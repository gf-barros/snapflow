{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load, YAMLError\n",
    "from src.utils import setup_output_folder\n",
    "from src.snapshots import snapshots_assembly, data_normalization, insert_h5_vector\n",
    "from src.linear_reduction import SVD\n",
    "from src.nonlinear_reduction import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 'loss_5000',\n",
       " 'random_state': 42,\n",
       " 'normalization': {'snapshots': None,\n",
       "  'svd': 'min_max',\n",
       "  'autoencoder': None,\n",
       "  'surrogate': None},\n",
       " 'splitting': {'strategy': 'temporal',\n",
       "  'number_of_folds_or_splits': 2,\n",
       "  'train_size': 0.8,\n",
       "  'validation_size': 300,\n",
       "  'test_size': 0.1,\n",
       "  'gap': 0},\n",
       " 'snapshots': {'file_type_str': 'h5_fenics',\n",
       "  'folder': 'data/input',\n",
       "  'visualization_folder': 'data/visualization',\n",
       "  'file_name_contains': ['concentration'],\n",
       "  'dataset': None},\n",
       " 'svd': {'trunc_basis': 27,\n",
       "  'normalization': 'min_max',\n",
       "  'svd_type': 'randomized_svd',\n",
       "  'power_iterations': 1,\n",
       "  'oversampling': 20},\n",
       " 'auto_encoder': {'batch_size': 300,\n",
       "  'num_epochs': 1,\n",
       "  'learning_rate': '1e-4',\n",
       "  'weight_decay': '1e-8',\n",
       "  'loss_function': 'smooth_l1_loss',\n",
       "  'loss_parameters': {'beta': 0.2},\n",
       "  'num_workers': 2,\n",
       "  'number_of_hidden_layers': 5,\n",
       "  'hidden_layers_sizes': [256, 128, 64, 32, 16],\n",
       "  'hidden_layers_activation_function': ['leaky_relu',\n",
       "   'leaky_relu',\n",
       "   'leaky_relu',\n",
       "   'leaky_relu',\n",
       "   ''],\n",
       "  'hidden_layers_activation_function_parameters': [0.2, 0.2, 0.2, 0.2, 'None'],\n",
       "  'decoder_activation_function': 'sigmoid',\n",
       "  'decoder_activation_function_parameter': 'None'}}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames, snapshots = snapshots_assembly(params[\"snapshots\"])\n",
    "snapshots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def pipeline_modes():\n",
    "    # setup directories\n",
    "    output_folder = setup_output_folder(params)\n",
    "\n",
    "    # train_test split\n",
    "    data_splitter = DataSplitter(params)\n",
    "    train_data, test_data = data_splitter.preserve_test_data(data)\n",
    "    #data_splitter.split_data(snapshots)\n",
    "\n",
    "    # # high dimensional data\n",
    "    # svd = SVD(snapshots, params, output_folder)\n",
    "    # svd.fit()\n",
    "    # svd.plot_singular_values()\n",
    "    # spatial_modes = svd.u\n",
    "    # print(f\"spatial modes dim: {spatial_modes.shape}\")\n",
    "    # # preprocess high dimensional data\n",
    "    # normalized_spatial_modes, u_normalization_obj = data_normalization(\n",
    "    # spatial_modes, params, \"svd\", transpose=False\n",
    "    # )    \n",
    "    # print(f\"normalized spatial modes dim: {normalized_spatial_modes.shape}\")\n",
    "    # # fit high dimensional data\n",
    "    # auto_encoder = AutoEncoder(normalized_spatial_modes, params, output_folder)\n",
    "    # auto_encoder.fit()\n",
    "    # auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataSplitter' object has no attribute 'preserve_test_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipeline_modes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[139], line 7\u001b[0m, in \u001b[0;36mpipeline_modes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# train_test split\u001b[39;00m\n\u001b[1;32m      6\u001b[0m data_splitter \u001b[38;5;241m=\u001b[39m DataSplitter(params)\n\u001b[0;32m----> 7\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_test_data\u001b[49m(data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataSplitter' object has no attribute 'preserve_test_data'"
     ]
    }
   ],
   "source": [
    "pipeline_modes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_step_randomized = SVD(snapshots, params[\"svd\"])\n",
    "svd_step_randomized.fit()\n",
    "svd_step_randomized.plot_singular_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_snapshots = svd_step_randomized.u.T @ snapshots\n",
    "\n",
    "projected_snapshots_normalized, u_normalization_obj = data_normalization(\n",
    "    projected_snapshots, params[\"svd\"], transpose=False\n",
    ")  # min_max vetores singulares a esquerda\n",
    "projected_snapshots_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u_normalized[0, :].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector = u_normalized[:, 0]\n",
    "# insert_h5_vector(\n",
    "#     vector,\n",
    "#     params[\"snapshots\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_test_2 = AutoEncoder(projected_snapshots_normalized, params[\"auto_encoder\"])\n",
    "ae_test_2.fit()\n",
    "\n",
    "# print(ae_test_2.auto_encoder.encoder)\n",
    "# print(ae_test_2.auto_encoder.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_test_2.plot_quantities_per_epoch(\"avg_loss_by_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a analise: erro do autoencoder (apenas)\n",
    "\n",
    "import torch\n",
    "\n",
    "f_original_autoencoder = projected_snapshots_normalized[:, 0]\n",
    "f_reconstructed = ae_test_2.auto_encoder.forward(\n",
    "    torch.tensor(f_original_autoencoder, dtype=torch.float32)\n",
    ")\n",
    "print(f_reconstructed.shape)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.linalg.norm(\n",
    "    f_original_autoencoder - f_reconstructed.detach().numpy()\n",
    ") / np.linalg.norm(f_original_autoencoder, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a analise: erro do snapshot\n",
    "f_reconstructed = ae_test_2.auto_encoder.forward(\n",
    "    torch.tensor(projected_snapshots_normalized.T, dtype=torch.float32)\n",
    ")\n",
    "f_reconstructed_numpy = f_reconstructed.detach().numpy().T\n",
    "projected_snapshots_return = u_normalization_obj.inverse_transform(\n",
    "    f_reconstructed_numpy\n",
    ")\n",
    "print(projected_snapshots_return.shape)\n",
    "print(svd_step_randomized.u.shape)\n",
    "snapshots_returned = svd_step_randomized.u @ projected_snapshots_return\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "snapshot_number = 2500\n",
    "\n",
    "np.linalg.norm(\n",
    "    snapshots[:, snapshot_number] - snapshots_returned[:, snapshot_number]\n",
    ") / np.linalg.norm(snapshots[:, snapshot_number], 2)\n",
    "\n",
    "vector = snapshots_returned[:, snapshot_number]\n",
    "insert_h5_vector(\n",
    "    vector,\n",
    "    params[\"snapshots\"],\n",
    ")\n",
    "\n",
    "vector = snapshots[:, snapshot_number]\n",
    "insert_h5_vector(vector, params[\"snapshots\"], vector_dir=\"original_vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: train_test_split\n",
    "# TODO: organizar plots, geração de erros e dados\n",
    "# TODO: jogar no google docs\n",
    "# TODO: surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate",
   "language": "python",
   "name": "turbiditos_surrogate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
