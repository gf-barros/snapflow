{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load, YAMLError\n",
    "from src.utils import setup_output_folder\n",
    "from src.snapshots import snapshots_assembly, data_normalization\n",
    "from src.linear_reduction import SVD\n",
    "from src.nonlinear_reduction import AutoEncoder\n",
    "from src.data_split import DataSplitter\n",
    "from src.postprocessing import compute_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_name': 'epochs_5000',\n",
       " 'random_state': 42,\n",
       " 'normalization': {'snapshots': None,\n",
       "  'svd': 'min_max',\n",
       "  'autoencoder': None,\n",
       "  'surrogate': None},\n",
       " 'splitting': {'strategy': 'kfold',\n",
       "  'number_of_folds_or_splits': 2,\n",
       "  'train_size': 0.8,\n",
       "  'validation_size': 0.1,\n",
       "  'test_size': 0.1,\n",
       "  'gap': 0},\n",
       " 'snapshots': {'file_type_str': 'h5_fenics',\n",
       "  'folder': 'data/input',\n",
       "  'visualization_folder': 'data/visualization',\n",
       "  'file_name_contains': ['concentration'],\n",
       "  'dataset': None},\n",
       " 'svd': {'trunc_basis': 300,\n",
       "  'normalization': 'min_max',\n",
       "  'svd_type': 'randomized_svd',\n",
       "  'power_iterations': 1,\n",
       "  'oversampling': 20},\n",
       " 'auto_encoder': {'batch_size': 300,\n",
       "  'num_epochs': 5000,\n",
       "  'learning_rate': '1e-4',\n",
       "  'weight_decay': '1e-8',\n",
       "  'loss_function': 'smooth_l1_loss',\n",
       "  'loss_parameters': {'beta': 0.2},\n",
       "  'num_workers': 2,\n",
       "  'number_of_hidden_layers': 5,\n",
       "  'hidden_layers_sizes': [256, 128, 64, 32, 16],\n",
       "  'hidden_layers_activation_function': ['leaky_relu',\n",
       "   'leaky_relu',\n",
       "   'leaky_relu',\n",
       "   'leaky_relu',\n",
       "   ''],\n",
       "  'hidden_layers_activation_function_parameters': [0.2, 0.2, 0.2, 0.2, 'None'],\n",
       "  'decoder_activation_function': 'sigmoid',\n",
       "  'decoder_activation_function_parameter': 'None'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70801, 3000)\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    print(snapshots.shape)\n",
    "except:\n",
    "    filenames, snapshots = snapshots_assembly(params[\"snapshots\"])\n",
    "    snapshots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_modes(backtest_flag = True, inference_flag = False):\n",
    "    # setup directories\n",
    "    output_folder = setup_output_folder(params)\n",
    "\n",
    "    # high dimensional data\n",
    "    svd = SVD(snapshots, params, output_folder)\n",
    "    svd.fit()\n",
    "    svd.plot_singular_values()\n",
    "    spatial_modes = svd.u\n",
    "    print(f\"spatial modes dim: {spatial_modes.shape}\")\n",
    "\n",
    "    # train_test split\n",
    "    data_splitter = DataSplitter(params)\n",
    "    folded_data = data_splitter.split_data(spatial_modes, train_test_flag=True)\n",
    "    total_train_data = folded_data[0][\"train\"]\n",
    "    total_test_data = folded_data[0][\"test\"] \n",
    "\n",
    "    print(\"train_data shape\", total_train_data.shape)\n",
    "    print(\"train_data type\", type(total_train_data))\n",
    "\n",
    "    if backtest_flag:\n",
    "        # train_val split\n",
    "        model_selection_data = data_splitter.split_data(total_train_data, train_test_flag=False)\n",
    "\n",
    "        # fold artifacts:\n",
    "        for fold in model_selection_data.keys():\n",
    "            fold_train_data = model_selection_data[fold][\"train\"]\n",
    "            fold_train_indices = model_selection_data[fold][\"train_indices\"]\n",
    "            fold_validation_data = model_selection_data[fold][\"validation\"]\n",
    "            fold_validation_indices = model_selection_data[fold][\"validation_indices\"]\n",
    "\n",
    "            # preprocess high dimensional data\n",
    "            normalized_spatial_train_modes, u_normalization_train_fold_obj = data_normalization(\n",
    "            fold_train_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            normalized_spatial_val_modes, u_normalization_val_fold_obj = data_normalization(\n",
    "            fold_validation_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            print(f\"normalized spatial train modes dim: {normalized_spatial_train_modes.shape}\")\n",
    "            print(f\"normalized spatial val modes dim: {normalized_spatial_val_modes.shape}\")\n",
    "\n",
    "            # fit high dimensional data\n",
    "            auto_encoder = AutoEncoder(normalized_spatial_train_modes, params, output_folder)\n",
    "            auto_encoder.fit()\n",
    "            auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n",
    "\n",
    "            # compute error for training data\n",
    "            normalized_train_predictions = auto_encoder.predict(normalized_spatial_train_modes)\n",
    "            train_predictions = u_normalization_train_fold_obj.inverse_transform(normalized_train_predictions)\n",
    "            compute_errors(fold, train_predictions, fold_train_data, fold_train_indices, output_folder, analysis_type=\"train\", modeling_type=\"backtest\")\n",
    "\n",
    "            # compute error for validation data\n",
    "            normalized_val_predictions = auto_encoder.predict(normalized_spatial_val_modes)\n",
    "            val_predictions = u_normalization_val_fold_obj.inverse_transform(normalized_val_predictions)\n",
    "            compute_errors(fold, val_predictions, fold_validation_data, fold_validation_indices, output_folder, analysis_type=\"validation\", modeling_type=\"backtest\")\n",
    "            \n",
    "\n",
    "    if inference_flag:\n",
    "            # train for all data\n",
    "            total_train_indices = folded_data[0][\"train_indices\"]\n",
    "            total_test_indices = folded_data[0][\"test_indices\"]\n",
    "\n",
    "            # normalize training and data\n",
    "            total_normalized_spatial_train_modes, u_normalization_total_train_obj = data_normalization(\n",
    "            total_train_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            total_normalized_spatial_test_modes, u_normalization_total_test_obj = data_normalization(\n",
    "            total_test_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            print(f\"normalized total spatial train modes dim: {total_normalized_spatial_train_modes.shape}\")\n",
    "\n",
    "            # fit high dimensional data\n",
    "            auto_encoder = AutoEncoder(total_normalized_spatial_train_modes, params, output_folder)\n",
    "            auto_encoder.fit()\n",
    "            auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n",
    "\n",
    "            # compute error for training data\n",
    "            total_normalized_train_predictions = auto_encoder.predict(total_normalized_spatial_train_modes)\n",
    "            total_train_predictions = u_normalization_total_train_obj.inverse_transform(total_normalized_train_predictions)\n",
    "            compute_errors(fold, total_train_predictions, 0, total_train_indices, output_folder, analysis_type=\"train\", modeling_type=\"inference\")\n",
    "\n",
    "            # compute error for test data\n",
    "            total_normalized_test_predictions = auto_encoder.predict(total_normalized_spatial_test_modes)\n",
    "            total_test_predictions = u_normalization_total_test_obj.inverse_transform(total_normalized_test_predictions)\n",
    "            compute_errors(fold, total_test_predictions, 0, total_test_indices, output_folder, analysis_type=\"test\", modeling_type=\"inference\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pipeline_modes(inference_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: jogar no google docs\n",
    "# TODO: surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate_vscode",
   "language": "python",
   "name": "turbiditos_surrogate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
