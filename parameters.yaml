origin_experiment_name: input #["input", "yaml"]
experiment_name: "epochs_10000"
random_state: 42
normalization:
  snapshots: null
  svd: "min_max"
  autoencoder: null
  surrogate: null
splitting:
  strategy: "train_val" #["temporal", "kfold", "train_test", train_val]
  number_of_folds_or_splits: 5
  train_size: 0.8
  validation_size: 0.1
  test_size: 0.1
  gap: 0
snapshots:
  file_type_str: 'h5_fenics' #h5_libmesh
  folder: 'data/input'
  visualization_folder: 'data/visualization'
  file_name_contains: ['concentration']
  dataset: null
svd:
  trunc_basis: 30
  normalization: "min_max"
  svd_type: "randomized_svd"  # ["full_svd", "randomized_svd"]
  power_iterations: 1
  oversampling: 20
auto_encoder:
  batch_size: 30
  num_epochs: 10000
  learning_rate: 1e-4
  weight_decay: 1e-8
  loss_function: "smooth_l1_loss"
  loss_parameters: 
    beta: 0.2
  num_workers: 2
  number_of_hidden_layers: 5
  hidden_layers_sizes: [256, 128, 64, 32, 16]
  hidden_layers_activation_function: ["leaky_relu", "leaky_relu", "leaky_relu", "leaky_relu", ""]
  hidden_layers_activation_function_parameters: [0.2, 0.2, 0.2, 0.2, None]
  decoder_activation_function: "sigmoid"
  decoder_activation_function_parameter: None