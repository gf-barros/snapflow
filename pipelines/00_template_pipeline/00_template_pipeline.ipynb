{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef9154f",
   "metadata": {},
   "source": [
    "## Pipeline template\n",
    "\n",
    "Both Jupyter Notebook and Python script should have the same code. Sync is done via Jupytext library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c9fb8c",
   "metadata": {},
   "source": [
    "### Preamble\n",
    "\n",
    "This preamble is required for every pipeline, regardless of being executed via Jupyter Notebook or Python script. It is responsible for adding the root directory into the system PATH during execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8dc889ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_working_directory = Path.cwd()\n",
    "root_directory = current_working_directory.parent.parent\n",
    "sys.path.append(str(root_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75686c6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Loading the modules\n",
    "Modules should be loaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3cdb3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load, YAMLError\n",
    "from snapflow.utils import setup_output_folder, timing_decorator\n",
    "from snapflow.snapshots import snapshots_assembly, data_normalization\n",
    "from snapflow.linear_reduction import SVD\n",
    "from snapflow.nonlinear_reduction import AutoEncoder\n",
    "from snapflow.data_split import DataSplitter\n",
    "from snapflow.postprocessing import compute_errors, save_paraview_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b305eb9",
   "metadata": {},
   "source": [
    "### Reading the YAML file containing the parameters:\n",
    "\n",
    "Every pipeline should have its own parameters YAML file following the one presented in this template. It should be read using the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a4b63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a63f1",
   "metadata": {},
   "source": [
    "### Experiment name\n",
    "Notice that each pipeline can have multiple experiments. Each experiment should have its own name for output dumping purposes. If the `origin_experiment_name` key on the parameters file returns `input` (specially for debugging), the terminal will request a name for that experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8aa6c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"origin_experiment_name\"] == \"input\":\n",
    "     params[\"experiment_name\"] = input(\"Experiment name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce5fabdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'origin_experiment_name': 'yaml',\n",
       " 'experiment_name': 'template_pipeline',\n",
       " 'random_state': 42,\n",
       " 'snapshots': {'file_type_str': 'numpy',\n",
       "  'folder': 'data/input',\n",
       "  'file_name_contains': ['snapshots'],\n",
       "  'dataset': None},\n",
       " 'splitting': {'strategy': 'train_val',\n",
       "  'number_of_folds_or_splits': 5,\n",
       "  'train_size': 0.8,\n",
       "  'validation_size': 0.1,\n",
       "  'test_size': 0.1,\n",
       "  'gap': 0},\n",
       " 'normalization': {'snapshots': None,\n",
       "  'svd': None,\n",
       "  'auto_encoder': 'min_max',\n",
       "  'surrogate': 'min_max'},\n",
       " 'svd': {'trunc_basis': 30,\n",
       "  'normalization': 'min_max',\n",
       "  'svd_type': 'randomized_svd',\n",
       "  'power_iterations': 1,\n",
       "  'oversampling': 20},\n",
       " 'auto_encoder': {'data_loader': 'data_loader',\n",
       "  'data_loader_parameters': {'active': True,\n",
       "   'parameters': {'batch_size': 30, 'num_workers': 2}},\n",
       "  'num_epochs': 1,\n",
       "  'initializer': 'kaiming_normal',\n",
       "  'initializer_parameters': {'active': True,\n",
       "   'parameters': {'mode': 'fan_in', 'nonlinearity': 'leaky_relu'}},\n",
       "  'optimizer': 'adam',\n",
       "  'optimizer_parameters': {'active': True,\n",
       "   'parameters': {'lr': 0.0001, 'weight_decay': 1e-08}},\n",
       "  'loss_function': 'smooth_l1_loss',\n",
       "  'loss_function_parameters': {'active': True, 'parameters': {'beta': 0.2}},\n",
       "  'number_of_hidden_layers': 5,\n",
       "  'hidden_layers_sizes': {0: 256, 1: 128, 2: 64, 3: 32, 4: 16},\n",
       "  'hidden_layers_activation_function': {0: 'leaky_relu',\n",
       "   1: 'leaky_relu',\n",
       "   2: 'leaky_relu',\n",
       "   3: 'leaky_relu'},\n",
       "  'hidden_layers_activation_function_parameters': {'active': {0: True,\n",
       "    1: True,\n",
       "    2: True,\n",
       "    3: True},\n",
       "   'parameters': {0: {'negative_slope': 0.2},\n",
       "    1: {'negative_slope': 0.2},\n",
       "    2: {'negative_slope': 0.2},\n",
       "    3: {'negative_slope': 0.2}}},\n",
       "  'decoder_activation_function': 'sigmoid',\n",
       "  'decoder_activation_function_parameter': None},\n",
       " 'neural_network': {'data_loader': 'data_loader',\n",
       "  'data_loader_parameters': {'active': True,\n",
       "   'parameters': {'batch_size': 30, 'num_workers': 2}},\n",
       "  'num_epochs': 1,\n",
       "  'initializer': 'kaiming_normal',\n",
       "  'initializer_parameters': {'active': True,\n",
       "   'parameters': {'mode': 'fan_in', 'nonlinearity': 'leaky_relu'}},\n",
       "  'optimizer': 'adam',\n",
       "  'optimizer_parameters': {'active': True, 'parameters': {'lr': 0.005}},\n",
       "  'adaptive_learning_parameters': {'active': True,\n",
       "   'parameters': {'step_size': 50, 'gamma': 0.75}},\n",
       "  'loss_function': 'mse_loss',\n",
       "  'loss_function_parameters': {'active': False, 'parameters': None},\n",
       "  'num_workers': 2,\n",
       "  'number_of_hidden_layers': 5,\n",
       "  'hidden_layers_sizes': {0: 50, 1: 50, 2: 50, 3: 50, 4: 50},\n",
       "  'hidden_layers_activation_function': {0: 'sigmoid',\n",
       "   1: 'sigmoid',\n",
       "   2: 'sigmoid',\n",
       "   3: 'sigmoid',\n",
       "   4: 'sigmoid'},\n",
       "  'hidden_layers_activation_function_parameters': {'active': {0: False,\n",
       "    1: False,\n",
       "    2: False,\n",
       "    3: False,\n",
       "    4: False},\n",
       "   'parameters': {0: None, 1: None, 2: None, 3: None, 4: None}}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c297efd",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "For snapshots existent in the simulation output files, the `fenics_h5` and `libmesh_h5` files. Also, loading `.npy` is possible. If data is available in any other file type, the pipeline can be used as long as the snapshots are stacked on a $n \\times m$ matrix, where `n` the spatial discretization of the vector and `m` is the number of snapshots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08b49c50",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type_str numpy\n"
     ]
    }
   ],
   "source": [
    "filenames, snapshots = snapshots_assembly(params[\"snapshots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd077591",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: jogar no google docs\n",
    "# TODO: Save e load do modelo\n",
    "# TODO: Plotters e cálculos de erros devem ser classe?\n",
    "# TODO: plots de erro estão errados\n",
    "# TODO: surrogate\n",
    "# TODO: create_pipeline script -> create folders, gitkeeps, headers, notebooks and scripts with preambles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
