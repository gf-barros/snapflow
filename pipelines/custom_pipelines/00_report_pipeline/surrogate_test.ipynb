{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bfe55d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a294b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f06dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_working_directory = Path.cwd()\n",
    "root_directory = current_working_directory.parent.parent.parent\n",
    "sys.path.append(str(root_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bf36ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/gabriel/Documents/dev/data/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from yaml import safe_load, YAMLError\n",
    "from snapflow.utils import setup_output_folder, timing_decorator\n",
    "from snapflow.snapshots import snapshots_assembly, snapshots_assembly_multiple_folders\n",
    "from snapflow.normalization import data_normalization\n",
    "from snapflow.linear_reduction import SVD\n",
    "from snapflow.nonlinear_reduction import AutoEncoder\n",
    "from snapflow.surrogate import NeuralNetwork\n",
    "from snapflow.data_split import DataSplitter\n",
    "from snapflow.postprocessing import PostProcessing, save_paraview_visualization\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "output_dir = os.environ.get('CODING_DATA_DIR')\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ca1ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gabriel/Documents/dev/data/snapflow/00_report_pipeline/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#/Users/gabriel/Documents/dev/data/snapflow/00_report_pipeline\n",
    "output_dir += \"snapflow/00_report_pipeline/\"\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e9a483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2e77a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"origin_experiment_name\"] == \"input\":\n",
    "     params[\"experiment_name\"] = input(\"Experiment name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdf638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_to_be_trained = [0, 2, 4, 6, 8]\n",
    "train_folders_list = [\"theta\" + str(theta) for theta in angles_to_be_trained]\n",
    "angles_to_be_tested = [10]\n",
    "test_folders_list = [\"theta\" + str(theta) for theta in angles_to_be_tested]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d64db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Snapshots 2999: 100%|██████████| 2999/2999 [00:13<00:00, 220.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train snapshots size (70801, 3000)\n"
     ]
    }
   ],
   "source": [
    "filenames, train_snapshots = snapshots_assembly_multiple_folders(params[\"snapshots\"], \n",
    "                                                                 stride=5, \n",
    "                                                                 folders_list=train_folders_list, \n",
    "                                                                 local=params[\"local\"],\n",
    "                                                                 export_prefix=output_dir)\n",
    "print(\"train snapshots size\", train_snapshots.shape)\n",
    "number_of_train_time_steps = train_snapshots.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16500eff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Snapshots 599: 100%|██████████| 599/599 [00:01<00:00, 441.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test snapshots size (70801, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filenames, test_snapshots = snapshots_assembly_multiple_folders(params[\"snapshots\"], \n",
    "                                                                stride=5, \n",
    "                                                                folders_list=test_folders_list, \n",
    "                                                                local=params[\"local\"],\n",
    "                                                                export_prefix=output_dir)\n",
    "print(\"test snapshots size\", test_snapshots.shape)\n",
    "number_of_test_time_steps = test_snapshots.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370c47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = setup_output_folder(params, \n",
    "                                    local=params[\"local\"],\n",
    "                                    export_prefix=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3548a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test split\n",
    "data_splitter = DataSplitter(params)\n",
    "folded_data = data_splitter.split_data(train_snapshots, simple_split=True)\n",
    "train_data = folded_data[0][\"data\"]\n",
    "train_indices = folded_data[0][\"indices\"]\n",
    "train_spatial_indices = folded_data[0][\"spatial_indices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5525ab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train snapshots size (70801, 3000)\n",
      "train indices size (3000,)\n",
      "test snapshots size (70801, 600)\n",
      "test indices size (600,)\n"
     ]
    }
   ],
   "source": [
    "folded_data = data_splitter.split_data(test_snapshots, simple_split=True)\n",
    "test_data = folded_data[0][\"data\"]\n",
    "test_indices = folded_data[0][\"indices\"]\n",
    "test_spatial_indices = folded_data[0][\"spatial_indices\"]\n",
    "\n",
    "print(\"train snapshots size\", train_data.shape)\n",
    "print(\"train indices size\", train_indices.shape)\n",
    "print(\"test snapshots size\", test_data.shape)\n",
    "print(\"test indices size\", test_indices.shape)\n",
    "\n",
    "save_paraview_visualization(train_data[:, 0], \n",
    "                            output_folder, \n",
    "                            \"train_split_ic\", \n",
    "                            local=params[\"local\"],\n",
    "                            export_prefix=output_dir)\n",
    "save_paraview_visualization(test_data[:, 0], \n",
    "                            output_folder, \n",
    "                            \"test_split_ic\", \n",
    "                            local=params[\"local\"],\n",
    "                            export_prefix=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e609d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Reduction\n",
    "svd_train = SVD(train_data, params, output_folder=output_folder, analysis_type=\"train\")\n",
    "svd_train.fit()\n",
    "svd_train.plot_singular_values()\n",
    "save_paraview_visualization(svd_train.u[:, 0], \n",
    "                            output_folder, \n",
    "                            \"train_mode_0\", \n",
    "                            local=params[\"local\"],\n",
    "                            export_prefix=output_dir)\n",
    "\n",
    "projected_train_data = svd_train.u.T @ train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01d006ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized total projected train data dim: (270, 3000)\n"
     ]
    }
   ],
   "source": [
    "# normalize training and test data\n",
    "normalized_projected_train_data, normalization_projected_train_obj = data_normalization(\n",
    "projected_train_data, params, \"auto_encoder\", transpose=True\n",
    ")    \n",
    "print(f\"normalized total projected train data dim: {normalized_projected_train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c02ba33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:15<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# fit high dimensional data\n",
    "auto_encoder = AutoEncoder(normalized_projected_train_data, params, output_folder)\n",
    "auto_encoder.fit()\n",
    "auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n",
    "auto_encoder.save_model(local=params[\"local\"],\n",
    "                        export_prefix=output_dir)\n",
    "latent_train_data = auto_encoder.encode(normalized_projected_train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ceffe0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized total latent train data dim: (16, 3000)\n"
     ]
    }
   ],
   "source": [
    "# latent data transpose\n",
    "latent_train_data = latent_train_data.T\n",
    "\n",
    "# normalize training and test data\n",
    "normalized_latent_train_data, normalization_latent_train_obj = data_normalization(\n",
    "latent_train_data, params, \"surrogate\", transpose=True\n",
    ")    \n",
    "print(f\"normalized total latent train data dim: {normalized_latent_train_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f189d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn_train_data shape (3000, 2)\n",
      "nn_train_data shape (16, 3000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "# train surrogate\n",
    "nn_train_data = np.array([(angle, int(step)) for angle in angles_to_be_trained for step in range(number_of_train_time_steps//len(angles_to_be_trained))])\n",
    "nn_test_data = np.array([(angle, int(step)) for angle in angles_to_be_tested for step in range(number_of_test_time_steps//len(angles_to_be_tested))])\n",
    "\n",
    "print(\"nn_train_data shape\", nn_train_data.shape)\n",
    "print(\"nn_train_data shape\", normalized_latent_train_data.shape)\n",
    "neural_network = NeuralNetwork(nn_train_data, normalized_latent_train_data, params, output_folder)\n",
    "neural_network.fit()\n",
    "neural_network.save_model()\n",
    "neural_network.plot_quantities_per_epoch(\"avg_loss_by_epoch\", fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0186c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_latent_train_predictions shape (16, 3000)\n",
      "latent_train_predictions shape (3000, 16)\n",
      "normalized_decoded_train_predictions shape (270, 3000)\n",
      "decoded_train_predictions shape (3000, 270)\n",
      "decoded_train_predictions shape (3000, 270)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:09<00:00, 323.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70801,)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# compute error for training data\n",
    "normalized_latent_train_predictions = neural_network.predict(nn_train_data)\n",
    "print(\"normalized_latent_train_predictions shape\", normalized_latent_train_predictions.shape)\n",
    "\n",
    "latent_train_predictions = normalization_latent_train_obj.inverse_transform(normalized_latent_train_predictions.T)\n",
    "print(\"latent_train_predictions shape\", latent_train_predictions.shape)\n",
    "\n",
    "normalized_decoded_train_predictions = auto_encoder.decode(latent_train_predictions.T)\n",
    "print(\"normalized_decoded_train_predictions shape\", normalized_decoded_train_predictions.shape)\n",
    "\n",
    "decoded_train_predictions = normalization_projected_train_obj.inverse_transform(normalized_decoded_train_predictions.T)\n",
    "print(\"decoded_train_predictions shape\", decoded_train_predictions.shape)\n",
    "\n",
    "train_predictions = svd_train.u @ decoded_train_predictions.T\n",
    "print(\"decoded_train_predictions shape\", decoded_train_predictions.shape)\n",
    "\n",
    "postprocessing_train = PostProcessing(fold=0, \n",
    "                predictions=train_predictions, \n",
    "                ground_truth=train_data, \n",
    "                indices=train_indices, \n",
    "                spatial_indices=train_spatial_indices,\n",
    "                output_folder=output_folder, \n",
    "                params_dict=params,\n",
    "                analysis_type=\"train\", \n",
    "                modeling_type=\"inference\"\n",
    "                )\n",
    "postprocessing_train.compute_errors(local=params[\"local\"],\n",
    "                        export_prefix=output_dir)\n",
    "\n",
    "del train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e80795bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [00:00<00:00, 867.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70801,)\n",
      "float64\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# compute error for test data\n",
    "normalized_latent_test_predictions = neural_network.predict(nn_test_data)\n",
    "latent_test_predictions = normalization_latent_train_obj.inverse_transform(normalized_latent_test_predictions.T)\n",
    "normalized_decoded_test_predictions = auto_encoder.decode(latent_test_predictions.T)\n",
    "decoded_test_predictions = normalization_projected_train_obj.inverse_transform(normalized_decoded_test_predictions.T)\n",
    "test_predictions = svd_train.u @ decoded_test_predictions.T\n",
    "\n",
    "postprocessing_test = PostProcessing(fold=0, \n",
    "                predictions=test_predictions, \n",
    "                ground_truth=test_data, \n",
    "                indices=test_indices, \n",
    "                spatial_indices=test_spatial_indices,\n",
    "                params_dict=params,\n",
    "                output_folder=output_folder, \n",
    "                analysis_type=\"test\", \n",
    "                modeling_type=\"inference\"\n",
    "                )\n",
    "\n",
    "postprocessing_test.compute_errors(local=params[\"local\"],\n",
    "                        export_prefix=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
