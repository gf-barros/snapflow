{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1e1f2f",
   "metadata": {},
   "source": [
    "## Tutorial 01 - Autoencoder validation\n",
    "\n",
    "In this tutorial, we are going to test our implementations of the SVD factorization and the AutoEncoder.\n",
    "\n",
    "We start by running our mandatory preamble for properly defining directions to our source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdce66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "current_working_directory = Path.cwd()\n",
    "root_directory = current_working_directory.parent.parent.parent\n",
    "sys.path.append(str(root_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0defdf75",
   "metadata": {},
   "source": [
    "### Loading the modules for the pipeline\n",
    "\n",
    "Then, we load all modules required for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e94570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load, YAMLError\n",
    "from snapflow.utils import setup_output_folder, timing_decorator\n",
    "from snapflow.snapshots import snapshots_assembly, data_normalization\n",
    "from snapflow.linear_reduction import SVD\n",
    "from snapflow.nonlinear_reduction import AutoEncoder\n",
    "from snapflow.data_split import DataSplitter\n",
    "from snapflow.postprocessing import compute_errors, save_paraview_visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64958d2",
   "metadata": {},
   "source": [
    "### Load parameters file, define experiment name (if needed) and load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1dfc2",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "if params[\"origin_experiment_name\"] == \"input\":\n",
    "    params[\"experiment_name\"] = input(\"Experiment name: \")\n",
    "\n",
    "filenames, snapshots = snapshots_assembly(params[\"snapshots\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c4e3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Setup the pipeline\n",
    "\n",
    "Now, we create a function to execute the whole pipeline. The function itself is not mandatory, but it could be interesting to isolate the variables into the scope of the pipeline. We are going to time the execution of the pipeline using the `@timing_decorator`.\n",
    "\n",
    "In this pipeline, we setup two different approaches: the `backtest` and the `inference`. The backtest is often used when we want to evaluate the components of the pipeline separately. It might be necessary to change the ML model, perform hyperparameter tuning, validate hypothesis and so on. Once everything is setup, we can perform the inference, by training our selected model with the training data and testing with the test set. It is good practice to remove the `test` set during backtest, to avoid overfitting into the test set, leading to misleading performance of the model before deploying it into production.\n",
    "\n",
    "That is, whenever we are performing backtest, the `train` and `test` are already split at first. Then, during backtest, we split the total `train` data into `train` and `validation` data and perform our backtest. As soon as the model is chosen and defined, we can train the model with the total `train` data and analyze its performance with the `test` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa534e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def pipeline_modes(backtest_flag=True, inference_flag=False):\n",
    "    # setup directories\n",
    "    output_folder = setup_output_folder(params)\n",
    "\n",
    "    # train_test split\n",
    "    data_splitter = DataSplitter(params)\n",
    "    folded_data = data_splitter.split_data(snapshots, train_test_flag=True)\n",
    "    train_data = folded_data[0][\"train\"]\n",
    "    test_data = folded_data[0][\"test\"]\n",
    "    total_train_indices = folded_data[0][\"train_indices\"]\n",
    "    total_test_indices = folded_data[0][\"test_indices\"]\n",
    "    save_paraview_visualization(train_data[:, 0], output_folder, \"train_split_ic\")\n",
    "    save_paraview_visualization(test_data[:, 0], output_folder, \"test_split_ic\")\n",
    "\n",
    "    # First Reduction\n",
    "    svd_train = SVD(\n",
    "        train_data, params, output_folder=output_folder, analysis_type=\"train\"\n",
    "    )\n",
    "    svd_train.fit()\n",
    "    svd_train.plot_singular_values()\n",
    "    save_paraview_visualization(svd_train.u[:, 0], output_folder, \"train_mode_0\")\n",
    "\n",
    "    svd_test = SVD(test_data, params, output_folder=output_folder, analysis_type=\"test\")\n",
    "    svd_test.fit()\n",
    "    svd_test.plot_singular_values()\n",
    "    save_paraview_visualization(svd_train.u[:, 0], output_folder, \"test_mode_0\")\n",
    "\n",
    "    projected_train_data = svd_train.u.T @ train_data\n",
    "    projected_test_data = svd_test.u.T @ test_data\n",
    "\n",
    "    if backtest_flag:\n",
    "        # train_val split\n",
    "        model_selection_data = data_splitter.split_data(\n",
    "            projected_train_data, train_test_flag=False\n",
    "        )\n",
    "\n",
    "        # fold artifacts:\n",
    "        for fold in model_selection_data.keys():\n",
    "            fold_train_data = model_selection_data[fold][\"train\"]\n",
    "            fold_train_indices = model_selection_data[fold][\"train_indices\"]\n",
    "            fold_validation_data = model_selection_data[fold][\"validation\"]\n",
    "            fold_validation_indices = model_selection_data[fold][\"validation_indices\"]\n",
    "\n",
    "            # normalize training and validation data\n",
    "            normalized_projected_train_data, normalization_projected_train_obj = (\n",
    "                data_normalization(\n",
    "                    fold_train_data, params, \"auto_encoder\", transpose=False\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # fit high dimensional data\n",
    "            auto_encoder = AutoEncoder(\n",
    "                normalized_projected_train_data, params, output_folder\n",
    "            )\n",
    "            auto_encoder.fit()\n",
    "            auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\", fold)\n",
    "\n",
    "            # compute error for training data\n",
    "            normalized_train_predictions = auto_encoder.predict(\n",
    "                normalized_projected_train_data\n",
    "            )\n",
    "            train_predictions = normalization_projected_train_obj.inverse_transform(\n",
    "                normalized_train_predictions\n",
    "            )\n",
    "            high_dimensional_train_predictions = svd_train.u @ train_predictions\n",
    "            high_dimensional_fold_train_data = svd_train.u @ fold_train_data\n",
    "            save_paraview_visualization(\n",
    "                high_dimensional_train_predictions[:, 0],\n",
    "                output_folder,\n",
    "                f\"postprocessed_prediction_train_{fold_train_indices[0]}_fold_{fold}\",\n",
    "            )\n",
    "            save_paraview_visualization(\n",
    "                high_dimensional_fold_train_data[:, 0],\n",
    "                output_folder,\n",
    "                f\"postprocessed_train_data_{fold_train_indices[0]}_fold_{fold}\",\n",
    "            )\n",
    "            compute_errors(\n",
    "                fold,\n",
    "                high_dimensional_train_predictions,\n",
    "                high_dimensional_fold_train_data,\n",
    "                fold_train_indices,\n",
    "                output_folder,\n",
    "                analysis_type=\"train\",\n",
    "                modeling_type=\"backtest\",\n",
    "            )\n",
    "\n",
    "            # compute error for validation data\n",
    "            if fold_validation_data is not None:\n",
    "                (\n",
    "                    normalized_projected_validation_data,\n",
    "                    normalization_projected_validation_obj,\n",
    "                ) = data_normalization(\n",
    "                    fold_validation_data, params, \"auto_encoder\", transpose=False\n",
    "                )\n",
    "                normalized_validation_predictions = auto_encoder.predict(\n",
    "                    normalized_projected_validation_data\n",
    "                )\n",
    "                validation_predictions = (\n",
    "                    normalization_projected_validation_obj.inverse_transform(\n",
    "                        normalized_validation_predictions\n",
    "                    )\n",
    "                )\n",
    "                high_dimensional_validation_predictions = (\n",
    "                    svd_train.u @ validation_predictions\n",
    "                )\n",
    "                high_dimensional_fold_validation_data = (\n",
    "                    svd_train.u @ fold_validation_data\n",
    "                )\n",
    "                save_paraview_visualization(\n",
    "                    high_dimensional_validation_predictions[:, 0],\n",
    "                    output_folder,\n",
    "                    f\"postprocessed_prediction_validation_{fold_validation_indices[0]}_fold_{fold}\",\n",
    "                )\n",
    "                save_paraview_visualization(\n",
    "                    high_dimensional_fold_validation_data[:, 0],\n",
    "                    output_folder,\n",
    "                    f\"postprocessed_validation_data_{fold_validation_indices[0]}_fold_{fold}\",\n",
    "                )\n",
    "                compute_errors(\n",
    "                    fold,\n",
    "                    high_dimensional_validation_predictions,\n",
    "                    high_dimensional_fold_validation_data,\n",
    "                    fold_validation_indices,\n",
    "                    output_folder,\n",
    "                    analysis_type=\"validation\",\n",
    "                    modeling_type=\"backtest\",\n",
    "                )\n",
    "\n",
    "    if inference_flag:\n",
    "        # train for all data\n",
    "        total_train_indices = folded_data[0][\"train_indices\"]\n",
    "        total_test_indices = folded_data[0][\"test_indices\"]\n",
    "\n",
    "        # normalize training and data\n",
    "        total_normalized_spatial_train_modes, u_normalization_total_train_obj = (\n",
    "            data_normalization(total_train_data, params, \"svd\", transpose=False)\n",
    "        )\n",
    "        total_normalized_spatial_test_modes, u_normalization_total_test_obj = (\n",
    "            data_normalization(total_test_data, params, \"svd\", transpose=False)\n",
    "        )\n",
    "        print(\n",
    "            f\"normalized total spatial train modes dim: {total_normalized_spatial_train_modes.shape}\"\n",
    "        )\n",
    "\n",
    "        # fit high dimensional data\n",
    "        auto_encoder = AutoEncoder(\n",
    "            total_normalized_spatial_train_modes, params, output_folder\n",
    "        )\n",
    "        auto_encoder.fit()\n",
    "        auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n",
    "\n",
    "        # compute error for training data\n",
    "        total_normalized_train_predictions = auto_encoder.predict(\n",
    "            total_normalized_spatial_train_modes\n",
    "        )\n",
    "        total_train_predictions = u_normalization_total_train_obj.inverse_transform(\n",
    "            total_normalized_train_predictions\n",
    "        )\n",
    "        compute_errors(\n",
    "            fold,\n",
    "            total_train_predictions,\n",
    "            0,\n",
    "            total_train_indices,\n",
    "            output_folder,\n",
    "            analysis_type=\"train\",\n",
    "            modeling_type=\"inference\",\n",
    "        )\n",
    "\n",
    "        # compute error for test data\n",
    "        total_normalized_test_predictions = auto_encoder.predict(\n",
    "            total_normalized_spatial_test_modes\n",
    "        )\n",
    "        total_test_predictions = u_normalization_total_test_obj.inverse_transform(\n",
    "            total_normalized_test_predictions\n",
    "        )\n",
    "        compute_errors(\n",
    "            fold,\n",
    "            total_test_predictions,\n",
    "            0,\n",
    "            total_test_indices,\n",
    "            output_folder,\n",
    "            paraview_plot=\"first\",\n",
    "            analysis_type=\"test\",\n",
    "            modeling_type=\"inference\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pipeline_modes(inference_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99f9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: jogar no google docs\n",
    "# TODO: Save e load do modelo\n",
    "# TODO: Plotters e cálculos de erros devem ser classe?\n",
    "# TODO: plots de erro estão errados\n",
    "# TODO: surrogate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate_vscode",
   "language": "python",
   "name": "turbiditos_surrogate"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
