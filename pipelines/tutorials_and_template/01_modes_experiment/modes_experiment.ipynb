{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc889ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f83a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75686c6a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "current_working_directory = Path.cwd()\n",
    "root_directory = current_working_directory.parent.parent.parent\n",
    "sys.path.append(str(root_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdb3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import safe_load, YAMLError\n",
    "from snapflow.utils import setup_output_folder, timing_decorator\n",
    "from snapflow.snapshots import snapshots_assembly, data_normalization\n",
    "from snapflow.linear_reduction import SVD\n",
    "from snapflow.nonlinear_reduction import AutoEncoder\n",
    "from snapflow.data_split import DataSplitter\n",
    "from snapflow.postprocessing import compute_errors, save_paraview_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4b63e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"parameters.yaml\", \"r\") as stream:\n",
    "    try:\n",
    "        params = safe_load(stream)\n",
    "    except YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa6c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params[\"origin_experiment_name\"] == \"input\":\n",
    "     params[\"experiment_name\"] = input(\"Experiment name: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b49c50",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_type_str numpy\n"
     ]
    }
   ],
   "source": [
    "filenames, snapshots = snapshots_assembly(params[\"snapshots\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2007698e",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "@timing_decorator\n",
    "def pipeline_modes(backtest_flag = True, inference_flag = False):\n",
    "    # setup directories\n",
    "    output_folder = setup_output_folder(params)\n",
    "\n",
    "    # high dimensional data\n",
    "    svd = SVD(snapshots, params, output_folder)\n",
    "    svd.fit()\n",
    "    svd.plot_singular_values()\n",
    "    save_paraview_visualization(svd.u[:, 0], output_folder, \"original_mode_0\")\n",
    "\n",
    "    spatial_modes = svd.u\n",
    "    print(f\"spatial modes dim: {spatial_modes.shape}\")\n",
    "\n",
    "    # train_test split\n",
    "    data_splitter = DataSplitter(params)\n",
    "    folded_data = data_splitter.split_data(spatial_modes, train_test_flag=True)\n",
    "    total_train_data = folded_data[0][\"train\"]\n",
    "    total_test_data = folded_data[0][\"test\"] \n",
    "    save_paraview_visualization(total_train_data[:, 0], output_folder, \"train_test_split_mode_0\")\n",
    "    print(\"train_data shape\", total_train_data.shape)\n",
    "    print(\"train_data type\", type(total_train_data))\n",
    "\n",
    "    if backtest_flag:\n",
    "        # train_val split\n",
    "        model_selection_data = data_splitter.split_data(total_train_data, train_test_flag=False)\n",
    "\n",
    "        # fold artifacts:\n",
    "        for fold in model_selection_data.keys():\n",
    "            fold_train_data = model_selection_data[fold][\"train\"]\n",
    "            fold_train_indices = model_selection_data[fold][\"train_indices\"]\n",
    "            fold_validation_data = model_selection_data[fold][\"validation\"]\n",
    "            fold_validation_indices = model_selection_data[fold][\"validation_indices\"]\n",
    "            save_paraview_visualization(fold_train_data[:, 0], output_folder, \"train_val_split_mode_0\")\n",
    "\n",
    "            # preprocess high dimensional data\n",
    "            normalized_spatial_train_modes, u_normalization_train_fold_obj = data_normalization(\n",
    "            fold_train_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            save_paraview_visualization(normalized_spatial_train_modes[:, 0], output_folder, \"preprocessed_train_split_mode_0\")\n",
    "\n",
    "            # fit high dimensional data\n",
    "            auto_encoder = AutoEncoder(normalized_spatial_train_modes, params, output_folder)\n",
    "            auto_encoder.fit()\n",
    "            auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\", fold)\n",
    "\n",
    "            # compute error for training data\n",
    "            normalized_train_predictions = auto_encoder.predict(normalized_spatial_train_modes)\n",
    "            train_predictions = u_normalization_train_fold_obj.inverse_transform(normalized_train_predictions)\n",
    "            save_paraview_visualization(train_predictions[:, 0], output_folder, \"postprocessed_prediction_split_mode_0\")\n",
    "            compute_errors(fold, train_predictions, fold_train_data, fold_train_indices, output_folder, analysis_type=\"train\", modeling_type=\"backtest\")\n",
    "\n",
    "            # compute error for validation data\n",
    "            if fold_validation_data is not None:\n",
    "                normalized_spatial_val_modes, u_normalization_val_fold_obj = data_normalization(\n",
    "                fold_validation_data, params, \"svd\", transpose=False\n",
    "                )    \n",
    "                print(f\"normalized spatial train modes dim: {normalized_spatial_train_modes.shape}\")\n",
    "                print(f\"normalized spatial val modes dim: {normalized_spatial_val_modes.shape}\")\n",
    "                normalized_val_predictions = auto_encoder.predict(normalized_spatial_val_modes)\n",
    "                val_predictions = u_normalization_val_fold_obj.inverse_transform(normalized_val_predictions)\n",
    "                compute_errors(fold, val_predictions, fold_validation_data, fold_validation_indices, output_folder, analysis_type=\"validation\", modeling_type=\"backtest\")\n",
    "                \n",
    "\n",
    "    if inference_flag:\n",
    "            # train for all data\n",
    "            total_train_indices = folded_data[0][\"train_indices\"]\n",
    "            total_test_indices = folded_data[0][\"test_indices\"]\n",
    "\n",
    "            # normalize training and data\n",
    "            total_normalized_spatial_train_modes, u_normalization_total_train_obj = data_normalization(\n",
    "            total_train_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            total_normalized_spatial_test_modes, u_normalization_total_test_obj = data_normalization(\n",
    "            total_test_data, params, \"svd\", transpose=False\n",
    "            )    \n",
    "            print(f\"normalized total spatial train modes dim: {total_normalized_spatial_train_modes.shape}\")\n",
    "\n",
    "            # fit high dimensional data\n",
    "            auto_encoder = AutoEncoder(total_normalized_spatial_train_modes, params, output_folder)\n",
    "            auto_encoder.fit()\n",
    "            auto_encoder.plot_quantities_per_epoch(\"avg_loss_by_epoch\")\n",
    "\n",
    "            # compute error for training data\n",
    "            total_normalized_train_predictions = auto_encoder.predict(total_normalized_spatial_train_modes)\n",
    "            total_train_predictions = u_normalization_total_train_obj.inverse_transform(total_normalized_train_predictions)\n",
    "            compute_errors(fold, total_train_predictions, 0, total_train_indices, output_folder, analysis_type=\"train\", modeling_type=\"inference\")\n",
    "\n",
    "            # compute error for test data\n",
    "            total_normalized_test_predictions = auto_encoder.predict(total_normalized_spatial_test_modes)\n",
    "            total_test_predictions = u_normalization_total_test_obj.inverse_transform(total_normalized_test_predictions)\n",
    "            compute_errors(fold, total_test_predictions, 0, total_test_indices, output_folder, paraview_plot=\"first\", analysis_type=\"test\", modeling_type=\"inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aed8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pipeline_modes(inference_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd077591",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: jogar no google docs\n",
    "# TODO: Save e load do modelo\n",
    "# TODO: Plotters e cálculos de erros devem ser classe?\n",
    "# TODO: plots de erro estão errados\n",
    "# TODO: surrogate\n",
    "# TODO: create_pipeline script -> create folders, gitkeeps, headers, notebooks and scripts with preambles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a305ea",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turbiditos_surrogate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
